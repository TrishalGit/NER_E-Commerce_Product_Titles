{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-PYNLphcRx-"
      },
      "source": [
        "* Connecting to google drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MlSVWu9LeQYd",
        "outputId": "dc5670aa-69ee-4aa6-a317-dd594f0e16f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkU8vhMhcRyA"
      },
      "source": [
        "* Importing required libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "xH3Ige41cRyC"
      },
      "outputs": [],
      "source": [
        "from __future__ import print_function\n",
        "from collections import OrderedDict\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import init\n",
        "from torch.autograd import Variable\n",
        "from torch import autograd\n",
        "\n",
        "import time\n",
        "import _pickle as cPickle\n",
        "\n",
        "import urllib\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams['figure.dpi'] = 80\n",
        "plt.style.use('seaborn-pastel')\n",
        "\n",
        "import os\n",
        "import sys\n",
        "import codecs\n",
        "import re\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kpk4gkpycRyE"
      },
      "source": [
        "* Constants and parameters\n",
        "* We used all the fixed parameter and manipulated only 'embedding_path' where we generated our custom \n",
        "    word-embedding vectors based on GLOVE and 'dropout' to test dropout layer performance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "9VxvFj7EcRyE"
      },
      "outputs": [],
      "source": [
        "#parameters for the Model\n",
        "parameters = OrderedDict()\n",
        "parameters['train'] = \"/content/drive/MyDrive/Dataset/eng.trainMixed\" #Path to train file\n",
        "parameters['tag_scheme'] = \"BIOES\" #BIO or BIOES\n",
        "parameters['lower'] = True # Boolean variable to control lowercasing of words\n",
        "parameters['zeros'] =  False # Boolean variable to control replacement of  all digits by 0 \n",
        "parameters['char_dim'] = 30 #Char embedding dimension\n",
        "parameters['word_dim'] = 100 #Token embedding dimension\n",
        "parameters['word_lstm_dim'] = 200 #Token LSTM hidden layer size\n",
        "parameters['word_bidirect'] = True #Use a bidirectional LSTM for words\n",
        "parameters['embedding_path'] = \"/content/drive/MyDrive/Dataset/Custom.100d.txt\" #Location of pretrained embeddings\n",
        "parameters['all_emb'] = 1 #Load all embeddings\n",
        "parameters['crf'] = 1 #Use CRF (0 to disable)\n",
        "parameters['dropout'] = 0 #Droupout on the input (0 = no dropout)\n",
        "parameters['epoch'] =  50 #Number of epochs to run\"\n",
        "parameters['weights'] = \"\" #path to Pretrained for from a previous run\n",
        "parameters['name'] = \"self-trained-model\" # Model name\n",
        "parameters['gradient_clip']= 5.0\n",
        "parameters['char_mode']= \"CNN\"\n",
        "models_path = \"./models/\" #path to saved models\n",
        "\n",
        "#GPU\n",
        "parameters['use_gpu'] = torch.cuda.is_available() #GPU Check\n",
        "use_gpu = parameters['use_gpu']\n",
        "\n",
        "parameters['reload'] = \"./models/pre-trained-model\" \n",
        "\n",
        "#Constants\n",
        "START_TAG = '<START>'\n",
        "STOP_TAG = '<STOP>'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u6HOuJuvwMHu"
      },
      "source": [
        "* Creating directory to store best model while training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "t92JUprdcRyF"
      },
      "outputs": [],
      "source": [
        "#To store model\n",
        "name = parameters['name']\n",
        "model_name = models_path + name #get_name(parameters)\n",
        "\n",
        "if not os.path.exists(models_path):\n",
        "    os.makedirs(models_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deNbIFvWcRyH"
      },
      "source": [
        "* Load the training data into list of sentences\n",
        "* Replacing all digits present in the text to zeroes didn't improve our results on basic testing. So, we kept 'zeroes' parameter to false always."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "LWrk0_MtcRyH"
      },
      "outputs": [],
      "source": [
        "def zero_digits(s):\n",
        "    \"\"\"\n",
        "    Replace every digit in a string by a zero.\n",
        "    \"\"\"\n",
        "    return re.sub('\\d', '0', s)\n",
        "\n",
        "def load_sentences(path, zeros):\n",
        "    \"\"\"\n",
        "    Load sentences. A line must contain at least a word and its tag.\n",
        "    Sentences are separated by empty lines.\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    sentence = []\n",
        "    for line in codecs.open(path, 'r', 'utf8'):\n",
        "        line = zero_digits(line.rstrip()) if zeros else line.rstrip()\n",
        "        if not line:\n",
        "            if len(sentence) > 0:\n",
        "                if 'DOCSTART' not in sentence[0][0]:\n",
        "                    sentences.append(sentence)\n",
        "                sentence = []\n",
        "        else:\n",
        "            word = line.split()\n",
        "            assert len(word) >= 2\n",
        "            sentence.append(word)\n",
        "    if len(sentence) > 0:\n",
        "        if 'DOCSTART' not in sentence[0][0]:\n",
        "            sentences.append(sentence)\n",
        "    return sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HOjembWEcRyI"
      },
      "outputs": [],
      "source": [
        "train_sentences = load_sentences(parameters['train'], parameters['zeros'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UBxQmHbcRyI"
      },
      "source": [
        "* The training samples are in BIO tagging format that are converted to BIOES tagging"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "Yv5y4Hm6cRyI"
      },
      "outputs": [],
      "source": [
        "def iob2(tags):\n",
        "    \"\"\"\n",
        "    Check that tags have a valid BIO format.\n",
        "    Tags in BIO1 format are converted to BIO2.\n",
        "    \"\"\"\n",
        "    for i, tag in enumerate(tags):\n",
        "        if tag == 'O':\n",
        "            continue\n",
        "        split = tag.split('-')\n",
        "        if len(split) != 2 or split[0] not in ['I', 'B']:  # I had to remove special characters from original tags to merge the keywords to pass this part of code\n",
        "            return False\n",
        "        if split[0] == 'B':\n",
        "            continue\n",
        "        elif i == 0 or tags[i - 1] == 'O':  # conversion IOB1 to IOB2\n",
        "            tags[i] = 'B' + tag[1:]\n",
        "        elif tags[i - 1][1:] == tag[1:]:\n",
        "            continue\n",
        "        else:  # conversion IOB1 to IOB2\n",
        "            tags[i] = 'B' + tag[1:]\n",
        "    return True\n",
        "\n",
        "def iob_iobes(tags):\n",
        "    \"\"\"\n",
        "    the function is used to convert\n",
        "    BIO -> BIOES tagging\n",
        "    \"\"\"\n",
        "    new_tags = []\n",
        "    for i, tag in enumerate(tags):\n",
        "        if tag == 'O':\n",
        "            new_tags.append(tag)\n",
        "        elif tag.split('-')[0] == 'B':\n",
        "            if i + 1 != len(tags) and \\\n",
        "               tags[i + 1].split('-')[0] == 'I':\n",
        "                new_tags.append(tag)\n",
        "            else:\n",
        "                new_tags.append(tag.replace('B-', 'S-'))\n",
        "        elif tag.split('-')[0] == 'I':\n",
        "            if i + 1 < len(tags) and \\\n",
        "                    tags[i + 1].split('-')[0] == 'I':\n",
        "                new_tags.append(tag)\n",
        "            else:\n",
        "                new_tags.append(tag.replace('I-', 'E-'))\n",
        "        else:\n",
        "            raise Exception('Invalid IOB format!')\n",
        "    return new_tags\n",
        "\n",
        "def update_tag_scheme(sentences, tag_scheme):\n",
        "    \"\"\"\n",
        "    Check and update sentences tagging scheme to BIO2\n",
        "    Only BIO1 and BIO2 schemes are accepted for input data.\n",
        "    \"\"\"\n",
        "    for i, s in enumerate(sentences):\n",
        "        tags = [w[-1] for w in s]\n",
        "        # Check that tags are given in the BIO format\n",
        "        if not iob2(tags):\n",
        "            s_str = '\\n'.join(' '.join(w) for w in s)\n",
        "            raise Exception('Sentences should be given in BIO format! ' +\n",
        "                            'Please check sentence %i:\\n%s' % (i, s_str))\n",
        "        if tag_scheme == 'BIOES':\n",
        "            new_tags = iob_iobes(tags)\n",
        "            for word, new_tag in zip(s, new_tags):\n",
        "                word[-1] = new_tag\n",
        "        else:\n",
        "            raise Exception('Wrong tagging scheme!')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "1E7fKh53cRyJ"
      },
      "outputs": [],
      "source": [
        "update_tag_scheme(train_sentences, parameters['tag_scheme'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e7f2wstwcRyK"
      },
      "source": [
        "* Assigning unique words, chars, and tags with numerical IDs. To be useful while doing tensor operations in the architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "LremS30AcRyL"
      },
      "outputs": [],
      "source": [
        "def create_dico(item_list):\n",
        "    \"\"\"\n",
        "    Create a dictionary of items from a list of list of items.\n",
        "    \"\"\"\n",
        "    assert type(item_list) is list\n",
        "    dico = {}\n",
        "    for items in item_list:\n",
        "        for item in items:\n",
        "            if item not in dico:\n",
        "                dico[item] = 1\n",
        "            else:\n",
        "                dico[item] += 1\n",
        "    return dico\n",
        "\n",
        "def create_mapping(dico):\n",
        "    \"\"\"\n",
        "    Create a mapping (item to ID / ID to item) from a dictionary.\n",
        "    Items are ordered by decreasing frequency.\n",
        "    \"\"\"\n",
        "    sorted_items = sorted(dico.items(), key=lambda x: (-x[1], x[0]))\n",
        "    id_to_item = {i: v[0] for i, v in enumerate(sorted_items)}\n",
        "    item_to_id = {v: k for k, v in id_to_item.items()}\n",
        "    return item_to_id, id_to_item\n",
        "\n",
        "def word_mapping(sentences, lower):\n",
        "    \"\"\"\n",
        "    Create a dictionary and a mapping of words, sorted by frequency.\n",
        "    \"\"\"\n",
        "    words = [[x[0].lower() if lower else x[0] for x in s] for s in sentences]\n",
        "    dico = create_dico(words)\n",
        "    dico['<UNK>'] = 10000000 #UNK tag for unknown words\n",
        "    word_to_id, id_to_word = create_mapping(dico)\n",
        "    print(\"Found %i unique words (%i in total)\" % (\n",
        "        len(dico), sum(len(x) for x in words)\n",
        "    ))\n",
        "    return dico, word_to_id, id_to_word\n",
        "\n",
        "def char_mapping(sentences):\n",
        "    \"\"\"\n",
        "    Create a dictionary and mapping of characters, sorted by frequency.\n",
        "    \"\"\"\n",
        "    chars = [\"\".join([w[0] for w in s]) for s in sentences]\n",
        "    dico = create_dico(chars)\n",
        "    char_to_id, id_to_char = create_mapping(dico)\n",
        "    print(\"Found %i unique characters\" % len(dico))\n",
        "    return dico, char_to_id, id_to_char\n",
        "\n",
        "def tag_mapping(sentences):\n",
        "    \"\"\"\n",
        "    Create a dictionary and a mapping of tags, sorted by frequency.\n",
        "    \"\"\"\n",
        "    tags = [[word[-1] for word in s] for s in sentences]\n",
        "    dico = create_dico(tags)\n",
        "    dico[START_TAG] = -1\n",
        "    dico[STOP_TAG] = -2\n",
        "    tag_to_id, id_to_tag = create_mapping(dico)\n",
        "    print(\"Found %i unique named entity tags\" % len(dico))\n",
        "    return dico, tag_to_id, id_to_tag"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-SqVOZVkcRyL",
        "outputId": "172cc1b8-1739-4854-bf13-e4c6384e8e73"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 2455 unique words (90102 in total)\n",
            "Found 57 unique characters\n",
            "Found 28 unique named entity tags\n"
          ]
        }
      ],
      "source": [
        "dico_words,word_to_id,id_to_word = word_mapping(train_sentences, parameters['lower'])\n",
        "dico_chars, char_to_id, id_to_char = char_mapping(train_sentences)\n",
        "dico_tags, tag_to_id, id_to_tag = tag_mapping(train_sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzAope3lcRyM"
      },
      "source": [
        "* Constructing final training dataset that contains information about the words in a training sample, its characters and the tags assigned to the words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "YSjPiPj6cRyN"
      },
      "outputs": [],
      "source": [
        "def lower_case(x,lower=False):\n",
        "    if lower:\n",
        "        return x.lower()  \n",
        "    else:\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "v1Pw6nf-cRyN"
      },
      "outputs": [],
      "source": [
        "def prepare_dataset(sentences, word_to_id, char_to_id, tag_to_id, lower=False):\n",
        "    \"\"\"\n",
        "    Prepare the dataset. Return a list of lists of dictionaries containing:\n",
        "        - word indexes\n",
        "        - word char indexes\n",
        "        - tag indexes\n",
        "    \"\"\"\n",
        "    data = []\n",
        "    for s in sentences:\n",
        "        str_words = [w[0] for w in s]\n",
        "        words = [word_to_id[lower_case(w,lower) if lower_case(w,lower) in word_to_id else '<UNK>']\n",
        "                 for w in str_words]\n",
        "        # Skip characters that are not in the training set\n",
        "        chars = [[char_to_id[c] for c in w if c in char_to_id]\n",
        "                 for w in str_words]\n",
        "        tags = [tag_to_id[w[-1]] for w in s]\n",
        "        data.append({\n",
        "            'str_words': str_words,\n",
        "            'words': words,\n",
        "            'chars': chars,\n",
        "            'tags': tags,\n",
        "        })\n",
        "    return data\n",
        "\n",
        "train_data = prepare_dataset(\n",
        "    train_sentences, word_to_id, char_to_id, tag_to_id, parameters['lower']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GunKhJQcRyO"
      },
      "source": [
        "* Load the pre trained word-embeddings. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ctpox4k-cRyO",
        "outputId": "fb4ca03d-9384-4272-c868-bbb379238f80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded 3784 pretrained embeddings.\n"
          ]
        }
      ],
      "source": [
        "all_word_embeds = {}\n",
        "for i, line in enumerate(codecs.open(parameters['embedding_path'], 'r', 'utf-8')):\n",
        "    s = line.strip().split()\n",
        "    if len(s) == parameters['word_dim'] + 1:\n",
        "        all_word_embeds[s[0]] = np.array([float(i) for i in s[1:]])\n",
        "\n",
        "word_embeds = np.random.uniform(-np.sqrt(0.06), np.sqrt(0.06), (len(word_to_id), parameters['word_dim']))\n",
        "\n",
        "match_count = 0\n",
        "for w in word_to_id:\n",
        "    if w in all_word_embeds:\n",
        "        word_embeds[word_to_id[w]] = all_word_embeds[w]\n",
        "        match_count += 1\n",
        "    elif w.lower() in all_word_embeds:\n",
        "        word_embeds[word_to_id[w]] = all_word_embeds[w.lower()]\n",
        "        match_count += 1\n",
        "\n",
        "print('Loaded %i pretrained embeddings.' % len(all_word_embeds))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mvi0hfjXcRyP"
      },
      "source": [
        "* Initializing char-embeddings based on paper."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "oLjpELkGcRyQ"
      },
      "outputs": [],
      "source": [
        "def init_embedding(input_embedding):\n",
        "    \"\"\"\n",
        "    Initialize embedding\n",
        "    \"\"\"\n",
        "    bias = np.sqrt(3.0 / input_embedding.size(1))\n",
        "    nn.init.uniform(input_embedding, -bias, bias)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZNjZXvQxcRyQ"
      },
      "source": [
        "Similar to the initialization above, except this is for the linear layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "NdzjEwLHcRyQ"
      },
      "outputs": [],
      "source": [
        "# Need to look into this paper why they have taken some specific values for linear transformation https://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf\n",
        "def init_linear(input_linear):\n",
        "    \"\"\"\n",
        "    Initialize linear transformation\n",
        "    \"\"\"\n",
        "    \n",
        "    bias = np.sqrt(6.0 / (input_linear.weight.size(0) + input_linear.weight.size(1)))\n",
        "    nn.init.uniform(input_linear.weight, -bias, bias)\n",
        "    if input_linear.bias is not None:\n",
        "        input_linear.bias.data.zero_()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DN95gMP7cRyQ"
      },
      "source": [
        "* Initializing the weights for Bi-LSTM layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "NqGep3lhcRyQ"
      },
      "outputs": [],
      "source": [
        "def init_lstm(input_lstm):\n",
        "    \"\"\"\n",
        "    Initialize lstm\n",
        "    \n",
        "    PyTorch weights parameters:\n",
        "    \n",
        "        weight_ih_l[k]: the learnable input-hidden weights of the k-th layer,\n",
        "            of shape `(hidden_size * input_size)` for `k = 0`. Otherwise, the shape is\n",
        "            `(hidden_size * hidden_size)`\n",
        "            \n",
        "        weight_hh_l[k]: the learnable hidden-hidden weights of the k-th layer,\n",
        "            of shape `(hidden_size * hidden_size)`            \n",
        "    \"\"\"\n",
        "    \n",
        "    # Weights init for forward layer\n",
        "    for ind in range(0, input_lstm.num_layers):\n",
        "        \n",
        "        ## Gets the weights Tensor from our model, for the input-hidden weights in our current layer\n",
        "        weight = eval('input_lstm.weight_ih_l' + str(ind))\n",
        "        \n",
        "        # Initialize the sampling range\n",
        "        sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
        "        \n",
        "        # Randomly sample from our samping range using uniform distribution and apply it to our current layer\n",
        "        nn.init.uniform(weight, -sampling_range, sampling_range)\n",
        "        \n",
        "        # Similar to above but for the hidden-hidden weights of the current layer\n",
        "        weight = eval('input_lstm.weight_hh_l' + str(ind))\n",
        "        sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
        "        nn.init.uniform(weight, -sampling_range, sampling_range)\n",
        "        \n",
        "        \n",
        "    # We do the above again, for the backward layer if we are using a bi-directional LSTM (our final model uses this)\n",
        "    if input_lstm.bidirectional:\n",
        "        for ind in range(0, input_lstm.num_layers):\n",
        "            weight = eval('input_lstm.weight_ih_l' + str(ind) + '_reverse')\n",
        "            sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
        "            nn.init.uniform(weight, -sampling_range, sampling_range)\n",
        "            weight = eval('input_lstm.weight_hh_l' + str(ind) + '_reverse')\n",
        "            sampling_range = np.sqrt(6.0 / (weight.size(0) / 4 + weight.size(1)))\n",
        "            nn.init.uniform(weight, -sampling_range, sampling_range)\n",
        "\n",
        "    # Bias initialization steps\n",
        "    \n",
        "    # We initialize them to zero except for the forget gate bias, which is initialized to 1\n",
        "    if input_lstm.bias:\n",
        "        for ind in range(0, input_lstm.num_layers):\n",
        "            bias = eval('input_lstm.bias_ih_l' + str(ind))\n",
        "            \n",
        "            # Initializing to zero\n",
        "            bias.data.zero_()\n",
        "            \n",
        "            # This is the range of indices for our forget gates for each LSTM cell\n",
        "            bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
        "            \n",
        "            #Similar for the hidden-hidden layer\n",
        "            bias = eval('input_lstm.bias_hh_l' + str(ind))\n",
        "            bias.data.zero_()\n",
        "            bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
        "            \n",
        "        # Similar to above, we do for backward layer if we are using a bi-directional LSTM \n",
        "        if input_lstm.bidirectional:\n",
        "            for ind in range(0, input_lstm.num_layers):\n",
        "                bias = eval('input_lstm.bias_ih_l' + str(ind) + '_reverse')\n",
        "                bias.data.zero_()\n",
        "                bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1\n",
        "                bias = eval('input_lstm.bias_hh_l' + str(ind) + '_reverse')\n",
        "                bias.data.zero_()\n",
        "                bias.data[input_lstm.hidden_size: 2 * input_lstm.hidden_size] = 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "d7SxYPvqcRyT"
      },
      "outputs": [],
      "source": [
        "def log_sum_exp(vec):\n",
        "    '''\n",
        "    This function calculates the score explained above for the forward algorithm\n",
        "    vec 2D: 1 * tagset_size\n",
        "    '''\n",
        "    max_score = vec[0, argmax(vec)]\n",
        "    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n",
        "    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n",
        "    \n",
        "def argmax(vec):\n",
        "    '''\n",
        "    This function returns the max index in a vector\n",
        "    '''\n",
        "    _, idx = torch.max(vec, 1)\n",
        "    return to_scalar(idx)\n",
        "\n",
        "def to_scalar(var):\n",
        "    '''\n",
        "    Function to convert pytorch tensor to a scalar\n",
        "    '''\n",
        "    return var.view(-1).data.tolist()[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "VI4mGgG_cRyU"
      },
      "outputs": [],
      "source": [
        "def score_sentences(self, feats, tags):\n",
        "    # tags is ground_truth, a list of ints, length is len(sentence)\n",
        "    # feats is a 2D tensor, len(sentence) * tagset_size\n",
        "    r = torch.LongTensor(range(feats.size()[0]))\n",
        "    if self.use_gpu:\n",
        "        r = r.cuda()\n",
        "        pad_start_tags = torch.cat([torch.cuda.LongTensor([self.tag_to_ix[START_TAG]]), tags])\n",
        "        pad_stop_tags = torch.cat([tags, torch.cuda.LongTensor([self.tag_to_ix[STOP_TAG]])])\n",
        "    else:\n",
        "        pad_start_tags = torch.cat([torch.LongTensor([self.tag_to_ix[START_TAG]]), tags])\n",
        "        pad_stop_tags = torch.cat([tags, torch.LongTensor([self.tag_to_ix[STOP_TAG]])])\n",
        "\n",
        "    score = torch.sum(self.transitions[pad_stop_tags, pad_start_tags]) + torch.sum(feats[r, tags])\n",
        "\n",
        "    return score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qE-4SS3LcRyV"
      },
      "outputs": [],
      "source": [
        "def forward_alg(self, feats):\n",
        "    '''\n",
        "    This function performs the forward algorithm explained above\n",
        "    '''\n",
        "    # calculate in log domain\n",
        "    # feats is len(sentence) * tagset_size\n",
        "    # initialize alpha with a Tensor with values all equal to -10000.\n",
        "    \n",
        "    # Do the forward algorithm to compute the partition function\n",
        "    init_alphas = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n",
        "    \n",
        "    # START_TAG has all of the score.\n",
        "    init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n",
        "    \n",
        "    # Wrap in a variable so that we will get automatic backprop\n",
        "    forward_var = autograd.Variable(init_alphas)\n",
        "    if self.use_gpu:\n",
        "        forward_var = forward_var.cuda()\n",
        "        \n",
        "    # Iterate through the sentence\n",
        "    for feat in feats:\n",
        "        # broadcast the emission score: it is the same regardless of\n",
        "        # the previous tag\n",
        "        emit_score = feat.view(-1, 1)\n",
        "        \n",
        "        # the ith entry of trans_score is the score of transitioning to\n",
        "        # next_tag from i\n",
        "        tag_var = forward_var + self.transitions + emit_score\n",
        "        \n",
        "        # The ith entry of next_tag_var is the value for the\n",
        "        # edge (i -> next_tag) before we do log-sum-exp\n",
        "        max_tag_var, _ = torch.max(tag_var, dim=1)\n",
        "        \n",
        "        # The forward variable for this tag is log-sum-exp of all the\n",
        "        # scores.\n",
        "        tag_var = tag_var - max_tag_var.view(-1, 1)\n",
        "        \n",
        "        # Compute log sum exp in a numerically stable way for the forward algorithm\n",
        "        forward_var = max_tag_var + torch.log(torch.sum(torch.exp(tag_var), dim=1)).view(1, -1) # ).view(1, -1)\n",
        "    terminal_var = (forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]).view(1, -1)\n",
        "    alpha = log_sum_exp(terminal_var)\n",
        "    # Z(x)\n",
        "    return alpha"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SzehcUB7wMHz"
      },
      "source": [
        "* Viterbi Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "xxto9HHicRyV"
      },
      "outputs": [],
      "source": [
        "def viterbi_algo(self, feats):\n",
        "    '''\n",
        "    In this function, we implement the viterbi algorithm explained above.\n",
        "    A Dynamic programming based approach to find the best tag sequence\n",
        "    '''\n",
        "    backpointers = []\n",
        "    # analogous to forward\n",
        "    \n",
        "    # Initialize the viterbi variables in log space\n",
        "    init_vvars = torch.Tensor(1, self.tagset_size).fill_(-10000.)\n",
        "    init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n",
        "    \n",
        "    # forward_var at step i holds the viterbi variables for step i-1\n",
        "    forward_var = Variable(init_vvars)\n",
        "    if self.use_gpu:\n",
        "        forward_var = forward_var.cuda()\n",
        "    for feat in feats:\n",
        "        next_tag_var = forward_var.view(1, -1).expand(self.tagset_size, self.tagset_size) + self.transitions\n",
        "        _, bptrs_t = torch.max(next_tag_var, dim=1)\n",
        "        bptrs_t = bptrs_t.squeeze().data.cpu().numpy() # holds the backpointers for this step\n",
        "        next_tag_var = next_tag_var.data.cpu().numpy() \n",
        "        viterbivars_t = next_tag_var[range(len(bptrs_t)), bptrs_t] # holds the viterbi variables for this step\n",
        "        viterbivars_t = Variable(torch.FloatTensor(viterbivars_t))\n",
        "        if self.use_gpu:\n",
        "            viterbivars_t = viterbivars_t.cuda()\n",
        "            \n",
        "        # Now add in the emission scores, and assign forward_var to the set\n",
        "        # of viterbi variables we just computed\n",
        "        forward_var = viterbivars_t + feat\n",
        "        backpointers.append(bptrs_t)\n",
        "\n",
        "    # Transition to STOP_TAG\n",
        "    terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n",
        "    terminal_var.data[self.tag_to_ix[STOP_TAG]] = -10000.\n",
        "    terminal_var.data[self.tag_to_ix[START_TAG]] = -10000.\n",
        "    best_tag_id = argmax(terminal_var.unsqueeze(0))\n",
        "    path_score = terminal_var[best_tag_id]\n",
        "    \n",
        "    # Follow the back pointers to decode the best path.\n",
        "    best_path = [best_tag_id]\n",
        "    for bptrs_t in reversed(backpointers):\n",
        "        best_tag_id = bptrs_t[best_tag_id]\n",
        "        best_path.append(best_tag_id)\n",
        "        \n",
        "    # Pop off the start tag (we dont want to return that to the caller)\n",
        "    start = best_path.pop()\n",
        "    assert start == self.tag_to_ix[START_TAG] # Sanity check\n",
        "    best_path.reverse()\n",
        "    return path_score, best_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "th6K4PYccRyY"
      },
      "outputs": [],
      "source": [
        "def forward_calc(self, sentence, chars, chars2_length, d):\n",
        "    \n",
        "    '''\n",
        "    The function calls viterbi decode and generates the \n",
        "    most probable sequence of tags for the sentence\n",
        "    '''\n",
        "    \n",
        "    # Get the emission scores from the BiLSTM\n",
        "    feats = self._get_lstm_features(sentence, chars, chars2_length, d)\n",
        "    # viterbi to get tag_seq\n",
        "    \n",
        "    # Find the best path, given the features.\n",
        "    if self.use_crf:\n",
        "        score, tag_seq = self.viterbi_decode(feats)\n",
        "    else:\n",
        "        score, tag_seq = torch.max(feats, 1)\n",
        "        tag_seq = list(tag_seq.cpu().data)\n",
        "\n",
        "    return score, tag_seq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "_DTD69_jcRyb"
      },
      "outputs": [],
      "source": [
        "def get_lstm_features(self, sentence, chars2, chars2_length, d):\n",
        "    \n",
        "    if self.char_mode == 'CNN':\n",
        "        chars_embeds = self.char_embeds(chars2).unsqueeze(1)\n",
        "\n",
        "        ## Creating Character level representation using Convolutional Neural Netowrk\n",
        "        ## followed by a Maxpooling Layer\n",
        "        chars_cnn_out3 = self.char_cnn3(chars_embeds)\n",
        "        chars_embeds = nn.functional.max_pool2d(chars_cnn_out3,\n",
        "                                             kernel_size=(chars_cnn_out3.size(2), 1)).view(chars_cnn_out3.size(0), self.out_channels)\n",
        "        # chars_embeds = nn.functional.max_pool1d(chars_cnn_out3, kernel_size=chars_cnn_out3.size(2)).view(chars_cnn_out3.size(0), self.out_channels)\n",
        "        # print(chars_embeds)\n",
        "\n",
        "        ## Loading word embeddings\n",
        "    embeds = self.word_embeds(sentence)\n",
        "\n",
        "    ## We concatenate the word embeddings and the character level representation\n",
        "    ## to create unified representation for each word\n",
        "    embeds = torch.cat((embeds, chars_embeds), 1)\n",
        "\n",
        "    embeds = embeds.unsqueeze(1)\n",
        "\n",
        "    ## Dropout on the unified embeddings\n",
        "    embeds = self.dropout(embeds)\n",
        "\n",
        "    ## Word lstm\n",
        "    ## Takes words as input and generates a output at each step\n",
        "    lstm_out, _ = self.lstm(embeds)\n",
        "\n",
        "    ## Reshaping the outputs from the lstm layer\n",
        "    lstm_out = lstm_out.view(len(sentence), self.hidden_dim*2)\n",
        "\n",
        "    ## Dropout on the lstm output\n",
        "    lstm_out = self.dropout(lstm_out)\n",
        "\n",
        "    ## Linear layer converts the ouput vectors to tag space\n",
        "    lstm_feats = self.hidden2tag(lstm_out)\n",
        "    \n",
        "    return lstm_feats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "9aoQO9HacRyd"
      },
      "outputs": [],
      "source": [
        "def get_neg_log_likelihood(self, sentence, tags, chars2, chars2_length, d):\n",
        "    feats = self._get_lstm_features(sentence, chars2, chars2_length, d)\n",
        "\n",
        "    if self.use_crf:\n",
        "        forward_score = self._forward_alg(feats)\n",
        "        gold_score = self._score_sentence(feats, tags)\n",
        "        return forward_score - gold_score\n",
        "    else:\n",
        "        tags = Variable(tags)\n",
        "        scores = nn.functional.cross_entropy(feats, tags)\n",
        "        return scores"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "VHCOoEJdcRye"
      },
      "outputs": [],
      "source": [
        "class BiLSTM_CRF(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim,\n",
        "                 char_to_ix=None, pre_word_embeds=None, char_out_dimension=30,char_embedding_dim=30, use_gpu=False\n",
        "                 , use_crf=True, char_mode='CNN'):\n",
        "        '''\n",
        "        Input parameters:\n",
        "                \n",
        "                vocab_size= Size of vocabulary (int)\n",
        "                tag_to_ix = Dictionary that maps NER tags to indices\n",
        "                embedding_dim = Dimension of word embeddings (int)\n",
        "                hidden_dim = The hidden dimension of the LSTM layer (int)\n",
        "                char_to_ix = Dictionary that maps characters to indices\n",
        "                pre_word_embeds = Numpy array which provides mapping from word embeddings to word indices\n",
        "                char_out_dimension = Output dimension from the CNN encoder for character\n",
        "                char_embedding_dim = Dimension of the character embeddings\n",
        "                use_gpu = defines availability of GPU, \n",
        "                    when True: CUDA function calls are made\n",
        "                    else: Normal CPU function calls are made\n",
        "                use_crf = parameter which decides if you want to use the CRF layer for output decoding\n",
        "        '''\n",
        "        \n",
        "        super(BiLSTM_CRF, self).__init__()\n",
        "        \n",
        "        #parameter initialization for the model\n",
        "        self.use_gpu = use_gpu\n",
        "        self.embedding_dim = embedding_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.tag_to_ix = tag_to_ix\n",
        "        self.use_crf = use_crf\n",
        "        self.tagset_size = len(tag_to_ix)\n",
        "        self.out_channels = char_out_dimension\n",
        "        self.char_mode = char_mode\n",
        "\n",
        "        if char_embedding_dim is not None:\n",
        "            self.char_embedding_dim = char_embedding_dim\n",
        "            \n",
        "            #Initializing the character embedding layer\n",
        "            self.char_embeds = nn.Embedding(len(char_to_ix), char_embedding_dim)\n",
        "            init_embedding(self.char_embeds.weight)\n",
        "            # print(len(self.char_embeds))\n",
        "            # print(self.char_embeds)\n",
        "            \n",
        "            #Performing LSTM encoding on the character embeddings\n",
        "            if self.char_mode == 'LSTM':\n",
        "                self.char_lstm = nn.LSTM(char_embedding_dim, char_lstm_dim, num_layers=1, bidirectional=True)\n",
        "                init_lstm(self.char_lstm)\n",
        "                \n",
        "            #Performing CNN encoding on the character embeddings\n",
        "            if self.char_mode == 'CNN':\n",
        "                self.char_cnn3 = nn.Conv2d(in_channels=1, out_channels=self.out_channels, kernel_size=(3, char_embedding_dim), padding=(2,0))\n",
        "                # self.char_cnn3 = nn.Conv1d(in_channels=1, out_channels=self.out_channels, kernel_size=3, padding=2)\n",
        "\n",
        "        #Creating Embedding layer with dimension of ( number of words * dimension of each word)\n",
        "        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n",
        "        if pre_word_embeds is not None:\n",
        "            #Initializes the word embeddings with pretrained word embeddings\n",
        "            self.pre_word_embeds = True\n",
        "            self.word_embeds.weight = nn.Parameter(torch.FloatTensor(pre_word_embeds))\n",
        "            print('Got Pre Word Embeddings..')\n",
        "        else:\n",
        "            self.pre_word_embeds = False\n",
        "    \n",
        "        #Initializing the dropout layer, with dropout specificed in parameters\n",
        "        self.dropout = nn.Dropout(parameters['dropout'])\n",
        "        \n",
        "        #Lstm Layer:\n",
        "        #input dimension: word embedding dimension + character level representation\n",
        "        #bidirectional=True, specifies that we are using the bidirectional LSTM\n",
        "        if self.char_mode == 'LSTM':\n",
        "            self.lstm = nn.LSTM(embedding_dim+char_lstm_dim*2, hidden_dim, bidirectional=True)\n",
        "        if self.char_mode == 'CNN':\n",
        "            self.lstm = nn.LSTM(embedding_dim+self.out_channels, hidden_dim, bidirectional=True)\n",
        "            # self.lstm = nn.LSTM(embedding_dim+self.out_channels, hidden_dim, 3, dropout=parameters['dropout'], bidirectional=True)\n",
        "        \n",
        "        #Initializing the lstm layer using predefined function for initialization\n",
        "        init_lstm(self.lstm)\n",
        "        \n",
        "        # Linear layer which maps the output of the bidirectional LSTM into tag space.\n",
        "        self.hidden2tag = nn.Linear(hidden_dim*2, self.tagset_size)\n",
        "        \n",
        "        #Initializing the linear layer using predefined function for initialization\n",
        "        init_linear(self.hidden2tag) \n",
        "\n",
        "        if self.use_crf:\n",
        "            # Matrix of transition parameters.  Entry i,j is the score of transitioning *to* i *from* j.\n",
        "            # Matrix has a dimension of (total number of tags * total number of tags)\n",
        "            self.transitions = nn.Parameter(\n",
        "                torch.zeros(self.tagset_size, self.tagset_size))\n",
        "            \n",
        "            # These two statements enforce the constraint that we never transfer\n",
        "            # to the start tag and we never transfer from the stop tag\n",
        "            self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n",
        "            self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n",
        "\n",
        "    #assigning the functions, which we have defined earlier\n",
        "    _score_sentence = score_sentences\n",
        "    _get_lstm_features = get_lstm_features\n",
        "    _forward_alg = forward_alg\n",
        "    viterbi_decode = viterbi_algo\n",
        "    neg_log_likelihood = get_neg_log_likelihood\n",
        "    forward = forward_calc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCYcrkNPcRyg",
        "outputId": "7c05707c-bbf7-409d-95cb-10e01e36efcd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Got Pre Word Embeddings..\n",
            "Model Initialized!!!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-14-db3aa24d91b1>:6: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
            "  nn.init.uniform(input_embedding, -bias, bias)\n",
            "<ipython-input-16-3cc13ee83c7e>:25: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
            "  nn.init.uniform(weight, -sampling_range, sampling_range)\n",
            "<ipython-input-16-3cc13ee83c7e>:30: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
            "  nn.init.uniform(weight, -sampling_range, sampling_range)\n",
            "<ipython-input-16-3cc13ee83c7e>:38: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
            "  nn.init.uniform(weight, -sampling_range, sampling_range)\n",
            "<ipython-input-16-3cc13ee83c7e>:41: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
            "  nn.init.uniform(weight, -sampling_range, sampling_range)\n",
            "<ipython-input-15-3bc5824c4833>:8: UserWarning: nn.init.uniform is now deprecated in favor of nn.init.uniform_.\n",
            "  nn.init.uniform(input_linear.weight, -bias, bias)\n"
          ]
        }
      ],
      "source": [
        "#creating the model using the Class defined above\n",
        "model = BiLSTM_CRF(vocab_size=len(word_to_id),\n",
        "                   tag_to_ix=tag_to_id,\n",
        "                   embedding_dim=parameters['word_dim'],\n",
        "                   hidden_dim=parameters['word_lstm_dim'],\n",
        "                   use_gpu=use_gpu,\n",
        "                   char_to_ix=char_to_id,\n",
        "                   pre_word_embeds=word_embeds,\n",
        "                   use_crf=parameters['crf'],\n",
        "                   char_mode=parameters['char_mode'])\n",
        "print(\"Model Initialized!!!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eXTVcJ2AcRyh",
        "outputId": "6cb2f3b1-cb1f-4772-a276-ffb57195731c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BiLSTM_CRF(\n",
              "  (char_embeds): Embedding(57, 30)\n",
              "  (char_cnn3): Conv2d(1, 30, kernel_size=(3, 30), stride=(1, 1), padding=(2, 0))\n",
              "  (word_embeds): Embedding(2455, 100)\n",
              "  (dropout): Dropout(p=0, inplace=False)\n",
              "  (lstm): LSTM(130, 200, bidirectional=True)\n",
              "  (hidden2tag): Linear(in_features=400, out_features=28, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "model.cuda()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vWQKYBIQcRyh"
      },
      "source": [
        "* Training Paramaters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "QSSDzI5XcRyi"
      },
      "outputs": [],
      "source": [
        "#Initializing the optimizer\n",
        "#The best results in the paper where achived using stochastic gradient descent (SGD) \n",
        "#learning rate=0.015 and momentum=0.9 \n",
        "#decay_rate=0.05 \n",
        "\n",
        "learning_rate = 0.015\n",
        "momentum = 0.9\n",
        "number_of_epochs = parameters['epoch'] \n",
        "decay_rate = 0.05\n",
        "gradient_clip = parameters['gradient_clip']\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, momentum=momentum)\n",
        "\n",
        "#variables which will used in training process\n",
        "losses = [] #list to store all losses\n",
        "loss = 0.0 #Loss Initializatoin\n",
        "best_train_F = -1.0 # Current best F-1 Score on Train Set\n",
        "all_F = [[0, 0, 0]] # List storing all the F-1 Scores\n",
        "eval_every = len(train_data) # Calculate F-1 Score after this many iterations\n",
        "plot_every = 2000 # Store loss after this many iterations\n",
        "count = 0 #Counts the number of iterations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "VssWiMlccRys"
      },
      "outputs": [],
      "source": [
        "def get_chunk_type(tok, idx_to_tag):\n",
        "    \"\"\"\n",
        "    The function takes in a chunk (\"B-PER\") and then splits it into the tag (PER) and its class (B)\n",
        "    as defined in BIOES\n",
        "    \n",
        "    Args:\n",
        "        tok: id of token, ex 4\n",
        "        idx_to_tag: dictionary {4: \"B-PER\", ...}\n",
        "\n",
        "    Returns:\n",
        "        tuple: \"B\", \"PER\"\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    tag_name = idx_to_tag[tok]\n",
        "    tag_class = tag_name.split('-')[0]\n",
        "    tag_type = tag_name.split('-')[-1]\n",
        "    return tag_class, tag_type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "DhfLwLcbc_gq"
      },
      "outputs": [],
      "source": [
        "def get_chunks(seq, tags):\n",
        "    \"\"\"Given a sequence of tags, group entities and their position\n",
        "\n",
        "    Args:\n",
        "        seq: [4, 4, 0, 0, ...] sequence of labels\n",
        "        tags: dict[\"O\"] = 4\n",
        "\n",
        "    Returns:\n",
        "        list of (chunk_type, chunk_start, chunk_end)\n",
        "\n",
        "    Example:\n",
        "        seq = [4, 5, 0, 3]\n",
        "        tags = {\"B-PER\": 4, \"I-PER\": 5, \"B-LOC\": 3}\n",
        "        result = [(\"PER\", 0, 2), (\"LOC\", 3, 4)]\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    # We assume by default the tags lie outside a named entity\n",
        "    default = tags[\"O\"]\n",
        "    \n",
        "    idx_to_tag = {idx: tag for tag, idx in tags.items()}\n",
        "    \n",
        "    chunks = []\n",
        "    \n",
        "    chunk_type, chunk_start = None, None\n",
        "    for i, tok in enumerate(seq):\n",
        "        # End of a chunk 1\n",
        "        if tok == default and chunk_type is not None:\n",
        "            # Add a chunk.\n",
        "            chunk = (chunk_type, chunk_start, i)\n",
        "            chunks.append(chunk)\n",
        "            chunk_type, chunk_start = None, None\n",
        "\n",
        "        # End of a chunk + start of a chunk!\n",
        "        elif tok != default:\n",
        "            tok_chunk_class, tok_chunk_type = get_chunk_type(tok, idx_to_tag)\n",
        "            if chunk_type is None:\n",
        "                # Initialize chunk for each entity\n",
        "                chunk_type, chunk_start = tok_chunk_type, i\n",
        "            elif tok_chunk_type != chunk_type or tok_chunk_class == \"B\":\n",
        "                # If chunk class is B, i.e., its a beginning of a new named entity\n",
        "                # or, if the chunk type is different from the previous one, then we\n",
        "                # start labelling it as a new entity\n",
        "                chunk = (chunk_type, chunk_start, i)\n",
        "                chunks.append(chunk)\n",
        "                chunk_type, chunk_start = tok_chunk_type, i\n",
        "        else:\n",
        "            pass\n",
        "\n",
        "    # end condition\n",
        "    if chunk_type is not None:\n",
        "        chunk = (chunk_type, chunk_start, len(seq))\n",
        "        chunks.append(chunk)\n",
        "\n",
        "    return chunks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmJKPMVJwMH1"
      },
      "source": [
        "* Get the chunks in the exact format that can be processed for output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "e30le2cUcRyt"
      },
      "outputs": [],
      "source": [
        "def get_chunks2(seq, tags, words):\n",
        "    # We assume by default the tags lie outside a named entity\n",
        "    default = tags[\"O\"]\n",
        "    \n",
        "    idx_to_tag = {idx: tag for tag, idx in tags.items()}\n",
        "    \n",
        "    chunks = []\n",
        "    \n",
        "    chunk_type, chunk_start = None, None\n",
        "    q = 0\n",
        "    word = []\n",
        "    for i, tok in enumerate(seq):\n",
        "        # End of a chunk 1\n",
        "        if tok == default and chunk_type is not None:\n",
        "            # Add a chunk.\n",
        "            chunk = (chunk_type, chunk_start, i - q)\n",
        "            chunks.append(chunk)\n",
        "            chunk_type, chunk_start = None, None\n",
        "            word.append(words[i])\n",
        "\n",
        "        # End of a chunk + start of a chunk!\n",
        "        elif tok != default:\n",
        "            tok_chunk_class, tok_chunk_type = get_chunk_type(tok, idx_to_tag)\n",
        "            if chunk_type is None:\n",
        "                chunk_type, chunk_start = tok_chunk_type, i - q\n",
        "                word.append(words[i])\n",
        "            elif tok_chunk_class == \"E\" or tok_chunk_class == \"I\":\n",
        "                word[chunk_start] += \" \" + words[i]\n",
        "                q += 1\n",
        "            else:\n",
        "                chunk = (chunk_type, chunk_start, i - q)\n",
        "                chunks.append(chunk)\n",
        "                chunk_type, chunk_start = tok_chunk_type, i - q\n",
        "                word.append(words[i])\n",
        "        else:\n",
        "            word.append(words[i])\n",
        "            pass\n",
        "\n",
        "    # end condition\n",
        "    if chunk_type is not None:\n",
        "        chunk = (chunk_type, chunk_start, len(seq) - q)\n",
        "        chunks.append(chunk)\n",
        "\n",
        "    return chunks, word"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b7pE3ZjVwMH1"
      },
      "source": [
        "* Calculate F1 score in between the training cycle to store best model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "gNZTn0e9cRyt"
      },
      "outputs": [],
      "source": [
        "def evaluating(model, datas, best_F,dataset=\"Train\"):\n",
        "    '''\n",
        "    The function takes as input the model, data and calcuates F-1 Score\n",
        "    It performs conditional updates \n",
        "     1) Flag to save the model \n",
        "     2) Best F-1 score\n",
        "    ,if the F-1 score calculated improves on the previous F-1 score\n",
        "    '''\n",
        "    # Initializations\n",
        "    prediction = [] # A list that stores predicted tags\n",
        "    save = False # Flag that tells us if the model needs to be saved\n",
        "    new_F = 0.0 # Variable to store the current F1-Score (may not be the best)\n",
        "    correct_preds, total_correct, total_preds = 0., 0., 0. # Count variables\n",
        "    \n",
        "    for data in datas:\n",
        "        ground_truth_id = data['tags']\n",
        "        words = data['str_words']\n",
        "        chars2 = data['chars']  \n",
        "        \n",
        "        if parameters['char_mode'] == 'CNN':\n",
        "            d = {} \n",
        "\n",
        "            # Padding the each word to max word size of that sentence\n",
        "            chars2_length = [len(c) for c in chars2]\n",
        "            char_maxl = max(chars2_length)\n",
        "            chars2_mask = np.zeros((len(chars2_length), char_maxl), dtype='int')\n",
        "            for i, c in enumerate(chars2):\n",
        "                chars2_mask[i, :chars2_length[i]] = c\n",
        "            chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
        "\n",
        "        dwords = Variable(torch.LongTensor(data['words']))\n",
        "        \n",
        "        # We are getting the predicted output from our model\n",
        "        if use_gpu:\n",
        "            val,out = model(dwords.cuda(), chars2_mask.cuda(), chars2_length, d)\n",
        "        else:\n",
        "            val,out = model(dwords, chars2_mask, chars2_length, d)\n",
        "        predicted_id = out\n",
        "    \n",
        "        \n",
        "        # We use the get chunks function defined above to get the true chunks\n",
        "        # and the predicted chunks from true labels and predicted labels respectively\n",
        "        lab_chunks      = set(get_chunks(ground_truth_id,tag_to_id))\n",
        "        lab_pred_chunks = set(get_chunks(predicted_id,\n",
        "                                         tag_to_id))\n",
        "\n",
        "        # Updating the count variables\n",
        "        correct_preds += len(lab_chunks & lab_pred_chunks)\n",
        "        total_preds   += len(lab_pred_chunks)\n",
        "        total_correct += len(lab_chunks)\n",
        "    \n",
        "    # Calculating the F1-Score\n",
        "    p   = correct_preds / total_preds if correct_preds > 0 else 0\n",
        "    r   = correct_preds / total_correct if correct_preds > 0 else 0\n",
        "    new_F  = 2 * p * r / (p + r) if correct_preds > 0 else 0\n",
        "\n",
        "    print(\"{}: new_F: {} best_F: {} \".format(dataset,new_F,best_F))\n",
        "    \n",
        "    # If our current F1-Score is better than the previous best, we update the best\n",
        "    # to current F1 and we set the flag to indicate that we need to checkpoint this model\n",
        "    \n",
        "    if new_F>best_F:\n",
        "        best_F=new_F\n",
        "        save=True\n",
        "\n",
        "    return best_F, new_F, save"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmKgE4ydcRyu"
      },
      "source": [
        "* Learning rate decay"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "kp9xeE4BcRyu"
      },
      "outputs": [],
      "source": [
        "def adjust_learning_rate(optimizer, lr):\n",
        "    \"\"\"\n",
        "    shrink learning rate\n",
        "    \"\"\"\n",
        "    for param_group in optimizer.param_groups:\n",
        "        param_group['lr'] = lr"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZx5qxaVcRyu"
      },
      "source": [
        "* Training Step"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "cmrJOGyxcRyu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "397789f5-4593-47d2-81ca-ab2ae246faa1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "**********Epoch Number*******   1\n",
            "2000 :  0.3689102183581239\n",
            "4000 :  0.21408707308594804\n",
            "**********Epoch Number*******   2\n",
            "6000 :  0.18482896144883654\n",
            "8000 :  0.16159194949434963\n",
            "10000 :  0.1506855709953733\n",
            "**********Epoch Number*******   3\n",
            "12000 :  0.13525333360420327\n",
            "14000 :  0.13034237495710677\n",
            "**********Epoch Number*******   4\n",
            "16000 :  0.12358013374838075\n",
            "18000 :  0.10902653402497296\n",
            "20000 :  0.11578871602422677\n",
            "Train: new_F: 0.9394018801795014 best_F: -1.0 \n",
            "Saving Model to  ./models/self-trained-model\n",
            "**********Epoch Number*******   5\n",
            "22000 :  0.10226568085446751\n",
            "24000 :  0.1013766827611936\n",
            "**********Epoch Number*******   6\n",
            "26000 :  0.09858220571141381\n",
            "28000 :  0.0928165917688402\n",
            "30000 :  0.0944815839350746\n",
            "**********Epoch Number*******   7\n",
            "32000 :  0.08469114945964201\n",
            "34000 :  0.08729886325187344\n",
            "**********Epoch Number*******   8\n",
            "36000 :  0.08246749582868168\n",
            "38000 :  0.07893478959418097\n",
            "40000 :  0.08103438056540488\n",
            "Train: new_F: 0.9485391820731967 best_F: 0.9394018801795014 \n",
            "Saving Model to  ./models/self-trained-model\n",
            "**********Epoch Number*******   9\n",
            "42000 :  0.06735097499756311\n",
            "44000 :  0.07120693823111297\n",
            "**********Epoch Number*******   10\n",
            "46000 :  0.06146877776293077\n",
            "48000 :  0.05991240456619791\n",
            "50000 :  0.06524478294944899\n",
            "**********Epoch Number*******   11\n",
            "52000 :  0.04817308959521284\n",
            "54000 :  0.05317195805853284\n",
            "**********Epoch Number*******   12\n",
            "56000 :  0.04578327350292062\n",
            "58000 :  0.03864438411213054\n",
            "60000 :  0.045654139721640875\n",
            "Train: new_F: 0.9711855780123795 best_F: 0.9485391820731967 \n",
            "Saving Model to  ./models/self-trained-model\n",
            "**********Epoch Number*******   13\n",
            "62000 :  0.029617094361300566\n",
            "64000 :  0.0354188983829302\n",
            "**********Epoch Number*******   14\n",
            "66000 :  0.02919525946037202\n",
            "68000 :  0.025482654783940985\n",
            "70000 :  0.02610803322710765\n",
            "**********Epoch Number*******   15\n",
            "72000 :  0.01717764801911498\n",
            "74000 :  0.02016822812919207\n",
            "**********Epoch Number*******   16\n",
            "76000 :  0.01650013632599338\n",
            "78000 :  0.012922446541772607\n",
            "80000 :  0.014856508595267699\n",
            "Train: new_F: 0.9920998603925434 best_F: 0.9711855780123795 \n",
            "Saving Model to  ./models/self-trained-model\n",
            "**********Epoch Number*******   17\n",
            "82000 :  0.008306016470515369\n",
            "84000 :  0.009815509073750613\n",
            "**********Epoch Number*******   18\n",
            "86000 :  0.00829001874008999\n",
            "88000 :  0.0060166900055691735\n",
            "90000 :  0.008372665109336081\n",
            "**********Epoch Number*******   19\n",
            "92000 :  0.004582267013751005\n",
            "94000 :  0.005985608909174087\n",
            "**********Epoch Number*******   20\n",
            "96000 :  0.004449787409201569\n",
            "98000 :  0.0031653909520283257\n",
            "100000 :  0.004736087927731408\n",
            "**********Epoch Number*******   21\n",
            "102000 :  0.002627497228950245\n",
            "104000 :  0.00464093687227381\n",
            "Train: new_F: 0.9980781550288277 best_F: 0.9920998603925434 \n",
            "Saving Model to  ./models/self-trained-model\n",
            "**********Epoch Number*******   22\n",
            "106000 :  0.0019935574418251515\n",
            "108000 :  0.002981149552684419\n",
            "110000 :  0.003607625626848527\n",
            "Train: new_F: 0.9982424730211396 best_F: 0.9980781550288277 \n",
            "Saving Model to  ./models/self-trained-model\n",
            "**********Epoch Number*******   23\n",
            "112000 :  0.002543812802998999\n",
            "114000 :  0.00290170617304362\n",
            "Train: new_F: 0.9984394763211064 best_F: 0.9982424730211396 \n",
            "Saving Model to  ./models/self-trained-model\n",
            "**********Epoch Number*******   24\n",
            "116000 :  0.0019609658724144387\n",
            "118000 :  0.0018182286790850311\n",
            "120000 :  0.0027226362008013856\n",
            "Train: new_F: 0.9988173455978976 best_F: 0.9984394763211064 \n",
            "Saving Model to  ./models/self-trained-model\n",
            "**********Epoch Number*******   25\n",
            "122000 :  0.001317706062367774\n",
            "124000 :  0.0021037048157788595\n",
            "Train: new_F: 0.9986858963829298 best_F: 0.9988173455978976 \n",
            "**********Epoch Number*******   26\n",
            "126000 :  0.0021538120899039126\n",
            "128000 :  0.0015601146821842464\n",
            "130000 :  0.0017443782165661112\n",
            "Train: new_F: 0.9988337138820902 best_F: 0.9988173455978976 \n",
            "Saving Model to  ./models/self-trained-model\n",
            "**********Epoch Number*******   27\n",
            "132000 :  0.0010277526692873621\n",
            "134000 :  0.001967925116303418\n",
            "Train: new_F: 0.9992279514430737 best_F: 0.9988337138820902 \n",
            "Saving Model to  ./models/self-trained-model\n",
            "**********Epoch Number*******   28\n",
            "136000 :  0.001219594827239452\n",
            "138000 :  0.0006673451098239338\n",
            "140000 :  0.002098876773821177\n",
            "Train: new_F: 0.9993100502677662 best_F: 0.9992279514430737 \n",
            "Saving Model to  ./models/self-trained-model\n",
            "**********Epoch Number*******   29\n",
            "142000 :  0.0012071736093905163\n",
            "144000 :  0.0010818860585413332\n",
            "Train: new_F: 0.9992279514430737 best_F: 0.9993100502677662 \n",
            "**********Epoch Number*******   30\n",
            "146000 :  0.0012212493672056528\n",
            "148000 :  0.000785408443670281\n",
            "150000 :  0.0012216155997656051\n",
            "Train: new_F: 0.9994250513347023 best_F: 0.9993100502677662 \n",
            "Saving Model to  ./models/self-trained-model\n",
            "**********Epoch Number*******   31\n",
            "152000 :  0.0004147063574057205\n",
            "154000 :  0.0012854213028667594\n",
            "Train: new_F: 0.9992936577032377 best_F: 0.9994250513347023 \n",
            "**********Epoch Number*******   32\n",
            "156000 :  0.0006960211609952922\n",
            "158000 :  0.000743491999749453\n",
            "160000 :  0.0010647539108091153\n",
            "Train: new_F: 0.9994414692643823 best_F: 0.9994250513347023 \n",
            "Saving Model to  ./models/self-trained-model\n",
            "**********Epoch Number*******   33\n",
            "162000 :  0.00041977890259992107\n",
            "164000 :  0.0004578953594290416\n",
            "Train: new_F: 0.9995564827438934 best_F: 0.9994414692643823 \n",
            "Saving Model to  ./models/self-trained-model\n",
            "**********Epoch Number*******   34\n",
            "166000 :  0.0010241844546836395\n",
            "168000 :  0.0005813341649845835\n",
            "170000 :  0.0003866142176552664\n",
            "Train: new_F: 0.9996221890040573 best_F: 0.9995564827438934 \n",
            "Saving Model to  ./models/self-trained-model\n",
            "**********Epoch Number*******   35\n",
            "172000 :  0.0005071363220235779\n",
            "174000 :  0.0004788726653433452\n",
            "Train: new_F: 0.9996714633024509 best_F: 0.9996221890040573 \n",
            "Saving Model to  ./models/self-trained-model\n",
            "**********Epoch Number*******   36\n",
            "176000 :  0.0003085606874772993\n",
            "178000 :  0.0004339518661984347\n",
            "180000 :  0.00038707335346661913\n",
            "Train: new_F: 0.9998357316512254 best_F: 0.9996714633024509 \n",
            "Saving Model to  ./models/self-trained-model\n",
            "**********Epoch Number*******   37\n",
            "182000 :  0.00027972633160966095\n",
            "184000 :  0.00042405737959296\n",
            "Train: new_F: 0.9997371706419607 best_F: 0.9998357316512254 \n",
            "**********Epoch Number*******   38\n",
            "186000 :  0.00024227263813773534\n",
            "188000 :  0.0003849043342943076\n",
            "190000 :  0.00023304792283109925\n",
            "Train: new_F: 0.9999671463302451 best_F: 0.9998357316512254 \n",
            "Saving Model to  ./models/self-trained-model\n",
            "**********Epoch Number*******   39\n",
            "192000 :  0.00010094067487427852\n",
            "194000 :  0.00020418469341489863\n",
            "Train: new_F: 0.9999014389907352 best_F: 0.9999671463302451 \n",
            "**********Epoch Number*******   40\n",
            "196000 :  0.00029297462809162113\n",
            "198000 :  0.00016484585133618235\n",
            "200000 :  0.000307741392143657\n",
            "Train: new_F: 0.9999671463302451 best_F: 0.9999671463302451 \n",
            "**********Epoch Number*******   41\n",
            "202000 :  0.00016370719903671695\n",
            "204000 :  0.00011491365499509374\n",
            "Train: new_F: 1.0 best_F: 0.9999671463302451 \n",
            "Saving Model to  ./models/self-trained-model\n",
            "**********Epoch Number*******   42\n",
            "206000 :  0.00020453788001403985\n",
            "208000 :  0.00015219852481172956\n",
            "210000 :  0.00013498602412492132\n",
            "Train: new_F: 0.9999342926604902 best_F: 1.0 \n",
            "**********Epoch Number*******   43\n",
            "212000 :  8.52451220367859e-05\n",
            "214000 :  0.00011372836308786443\n",
            "Train: new_F: 0.9999671463302451 best_F: 1.0 \n",
            "**********Epoch Number*******   44\n",
            "216000 :  0.0002036252993769012\n",
            "218000 :  0.000125346292132834\n",
            "220000 :  0.0001536334676109005\n",
            "Train: new_F: 0.9999671463302451 best_F: 1.0 \n",
            "**********Epoch Number*******   45\n",
            "222000 :  7.865304235984074e-05\n",
            "224000 :  0.00010465163624325377\n",
            "Train: new_F: 1.0 best_F: 1.0 \n",
            "**********Epoch Number*******   46\n",
            "226000 :  0.0001385971407715595\n",
            "228000 :  0.00017055770453396505\n",
            "230000 :  0.00010015801548743794\n",
            "Train: new_F: 1.0 best_F: 1.0 \n",
            "**********Epoch Number*******   47\n",
            "232000 :  0.00012995843974205798\n",
            "234000 :  7.022365346942371e-05\n",
            "Train: new_F: 1.0 best_F: 1.0 \n",
            "**********Epoch Number*******   48\n",
            "236000 :  0.00010723165333748531\n",
            "238000 :  9.310578685759869e-05\n",
            "240000 :  0.00011168255703587068\n",
            "Train: new_F: 1.0 best_F: 1.0 \n",
            "**********Epoch Number*******   49\n",
            "242000 :  7.780962935835166e-05\n",
            "244000 :  7.555725998664523e-05\n",
            "Train: new_F: 1.0 best_F: 1.0 \n",
            "3181.847149372101\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 480x320 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "if True:\n",
        "    tr = time.time()\n",
        "    model.train(True)\n",
        "    for epoch in range(1,number_of_epochs):\n",
        "        print('**********Epoch Number*******   ' + str(epoch))\n",
        "        for i, index in enumerate(np.random.permutation(len(train_data))):\n",
        "            count += 1\n",
        "            data = train_data[index]\n",
        "\n",
        "            ##gradient updates for each data entry\n",
        "            model.zero_grad()\n",
        "\n",
        "            sentence_in = data['words']\n",
        "            sentence_in = Variable(torch.LongTensor(sentence_in))\n",
        "            tags = data['tags']\n",
        "            chars2 = data['chars']\n",
        "            \n",
        "            if parameters['char_mode'] == 'LSTM':\n",
        "                chars2_sorted = sorted(chars2, key=lambda p: len(p), reverse=True)\n",
        "                d = {}\n",
        "                for i, ci in enumerate(chars2):\n",
        "                    for j, cj in enumerate(chars2_sorted):\n",
        "                        if ci == cj and not j in d and not i in d.values():\n",
        "                            d[j] = i\n",
        "                            continue\n",
        "                chars2_length = [len(c) for c in chars2_sorted]\n",
        "                char_maxl = max(chars2_length)\n",
        "                chars2_mask = np.zeros((len(chars2_sorted), char_maxl), dtype='int')\n",
        "                for i, c in enumerate(chars2_sorted):\n",
        "                    chars2_mask[i, :chars2_length[i]] = c\n",
        "                chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
        "            \n",
        "            if parameters['char_mode'] == 'CNN':\n",
        "\n",
        "                d = {}\n",
        "\n",
        "                ## Padding the each word to max word size of that sentence\n",
        "                chars2_length = [len(c) for c in chars2]\n",
        "                char_maxl = max(chars2_length)\n",
        "                chars2_mask = np.zeros((len(chars2_length), char_maxl), dtype='int')\n",
        "                for i, c in enumerate(chars2):\n",
        "                    chars2_mask[i, :chars2_length[i]] = c\n",
        "                chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
        "\n",
        "\n",
        "            targets = torch.LongTensor(tags)\n",
        "\n",
        "            #we calculate the negative log-likelihood for the predicted tags using the predefined function\n",
        "            if use_gpu:\n",
        "                neg_log_likelihood = model.neg_log_likelihood(sentence_in.cuda(), targets.cuda(), chars2_mask.cuda(), chars2_length, d)\n",
        "            else:\n",
        "                neg_log_likelihood = model.neg_log_likelihood(sentence_in, targets, chars2_mask, chars2_length, d)\n",
        "            loss += neg_log_likelihood.data.item() / len(data['words'])\n",
        "            neg_log_likelihood.backward()\n",
        "\n",
        "            #we use gradient clipping to avoid exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)\n",
        "            optimizer.step()\n",
        "\n",
        "            #Storing loss\n",
        "            if count % plot_every == 0:\n",
        "                loss /= plot_every\n",
        "                print(count, ': ', loss)\n",
        "                if losses == []:\n",
        "                    losses.append(loss)\n",
        "                losses.append(loss)\n",
        "                loss = 0.0\n",
        "\n",
        "            #Evaluating on Train, Test, Dev Sets\n",
        "            if count % (eval_every) == 0 and count > (eval_every * 20) or \\\n",
        "                    count % (eval_every*4) == 0 and count < (eval_every * 20):\n",
        "                model.train(False)\n",
        "                best_train_F, new_train_F, save = evaluating(model, train_data, best_train_F,\"Train\")\n",
        "                if save:\n",
        "                    print(\"Saving Model to \", model_name)\n",
        "                    torch.save(model.state_dict(), model_name)\n",
        "                all_F.append([new_train_F])\n",
        "                model.train(True)\n",
        "\n",
        "            #Performing decay on the learning rate\n",
        "            if count % len(train_data) == 0:\n",
        "                adjust_learning_rate(optimizer, lr=learning_rate/(1+decay_rate*count/len(train_data)))\n",
        "\n",
        "    print(time.time() - tr)\n",
        "    plt.plot(losses)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eYjWY8bcRyv"
      },
      "source": [
        "* Testing the model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(model_name))\n",
        "model_testing_sentences = []\n",
        "id = []\n",
        "count = 0;\n",
        "import csv\n",
        "with open('/content/drive/MyDrive/Dataset/testTitlesMixed.csv', mode ='r') as file:\n",
        "    csvFile = csv.reader(file, delimiter=\",\")\n",
        "    for lines in csvFile:\n",
        "        model_testing_sentences.append(lines[-1])\n",
        "        id.append(lines[0])\n",
        "\n",
        "tags = {}\n",
        "with open('/content/drive/MyDrive/Dataset/tagsMixed.csv', mode ='r') as file:\n",
        "    csvFile = csv.reader(file, delimiter=\",\")\n",
        "    for lines in csvFile:\n",
        "        tags[lines[0]] = lines[1]\n",
        "\n",
        "lower=parameters['lower']\n",
        "\n",
        "final_test_data = []\n",
        "for sentence in model_testing_sentences:\n",
        "    s=sentence.split()\n",
        "    str_words = [w for w in s]\n",
        "    words = [word_to_id[lower_case(w,lower) if lower_case(w,lower) in word_to_id else '<UNK>'] for w in str_words]\n",
        "    \n",
        "    # Skip characters that are not in the training set\n",
        "    chars = [[char_to_id[c] for c in w if c in char_to_id] for w in str_words]\n",
        "    \n",
        "    final_test_data.append({\n",
        "        'str_words': str_words,\n",
        "        'words': words,\n",
        "        'chars': chars,\n",
        "    })\n",
        "\n",
        "#prediction\n",
        "predictions = []\n",
        "print(\"Prediction:\")\n",
        "print(\"word : tag\")\n",
        "count = 0\n",
        "result = []\n",
        "for data in final_test_data:\n",
        "    words = data['str_words']\n",
        "    chars2 = data['chars']\n",
        "\n",
        "    d = {} \n",
        "    \n",
        "    # Padding the each word to max word size of that sentence\n",
        "    chars2_length = [len(c) for c in chars2]\n",
        "    char_maxl = max(chars2_length)\n",
        "    chars2_mask = np.zeros((len(chars2_length), char_maxl), dtype='int')\n",
        "    for i, c in enumerate(chars2):\n",
        "        chars2_mask[i, :chars2_length[i]] = c\n",
        "    chars2_mask = Variable(torch.LongTensor(chars2_mask))\n",
        "\n",
        "    dwords = Variable(torch.LongTensor(data['words']))\n",
        "\n",
        "    # We are getting the predicted output from our model\n",
        "    if use_gpu:\n",
        "        val,predicted_id = model(dwords.cuda(), chars2_mask.cuda(), chars2_length, d)\n",
        "    else:\n",
        "        val,predicted_id = model(dwords, chars2_mask, chars2_length, d)\n",
        "    pred_chunks, words = get_chunks2(predicted_id,tag_to_id, words)\n",
        "    temp_list_tags=['NA']*len(words)\n",
        "    for p in pred_chunks:\n",
        "        temp_list_tags[p[1]]=p[0]\n",
        "    for word,tag in zip(words,temp_list_tags):\n",
        "        if tag != 'NA':\n",
        "            line = [id[count], tags[tag], word]\n",
        "            result.append('\\t'.join(line))\n",
        "    count += 1\n",
        "\n",
        "with open('/content/drive/MyDrive/Dataset/resultMixed.tsv', 'w', encoding='utf-8') as my_file:\n",
        "    for text in result:\n",
        "        my_file.write(text + '\\n')"
      ],
      "metadata": {
        "id": "AeVT7N1Gb82s"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}